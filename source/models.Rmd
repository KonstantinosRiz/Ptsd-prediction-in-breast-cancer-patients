---
title: "Model development"
output:
  html_document:
    df_print: paged
---

This file is used for testing the development of the machine learning models

```{r, imports, echo=FALSE, include=FALSE}
library(smotefamily)
library(plyr)
library(dplyr)
library(ROSE)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(ModelMetrics)
library(caret)
library(themis)
library(ranger)
library(randomForest)
library(mice)
library(sos)
library(kernlab)
library(ramify)
library(Matrix)
```

Initializing and loading the preprocessed, clean dataset which includes:

1.  final_train_features: list of m datasets created during preprocessing (containing only the features)
2.  train_labels: the labels of the train datasets (same for all of them)
3.  test_features: dataframe with the features to be used for testing
4.  test_labels: the labels to be used for evaluating the models
5.  preprocessing_comments: detailed comments about the preprocessing procedure Use function print_comments to pretty print them
6.  config_list: named list containing the parameters used to preprocess the data-set

```{r, load_data, echo=TRUE}

# Absolute path
my_path <- r"{D:\Κωνσταντίνος Data\Σχολής\Διπλωματική Εργασία\Main\source}"
setwd(my_path)
# Relative path
clean_data_folder <- r"{..\dataset\preprocessed_results}"
data_file <- r"{\M6_1.RData}"
# label_name <- "M6_ptsd"
data_path <- paste(clean_data_folder, data_file, sep="")

load(data_path)

# Print the preprocessing comments
cat(paste(preprocessing_comments, collapse="\n\n"))

```

```{r, load_code, echo=TRUE}

source("models_config.R")
source("models_aux.R")

```

```{r, split, echo=TRUE}

# Find the optimal model for every imputed dataset
# for (i in 1:as.integer(config_list["number_of_imputed_datasets"]) {

data <- final_features[[1]]

# Split the dataset into train/test (technically the split has happened during the preprocessing,
# we are just grouping together)
train_features <- data[train_indices, ]
test_features <- data[-train_indices, ]

train_labels <- new_final_label[train_indices]
test_labels <- new_final_label[-train_indices]

one_hot_train_features <- model.matrix(~ . - 1, data = train_features)
one_hot_test_features <- model.matrix(~ . - 1, data = test_features)

numeric_train_features <- train_features %>% select_if(is.numeric)
numeric_test_features <- test_features %>% select_if(is.numeric)

# train_labels <- data.frame(train_labels)
# names(train_labels) <- label_name

```

Use recursive feature elimination to reduce the number of our features even more

```{r, rfe, echo=TRUE}

myFuncs <- rfFuncs
myFuncs$summary <- f2_summary
rfe_control <- rfeControl(functions = myFuncs,
                          method = "repeatedcv",
                          repeats = 5,
                          verbose = FALSE)


## Rfe on initial features

rfe <- rfe(train_features, train_labels, 
                   sizes = c(seq(10, 50, 10), 100, 150, ncol(data)),
                   rfeControl = rfe_control,
                   metric = "f2")
train_reduced <- train_features[, rfe$optVariables]
test_reduced <- test_features[, rfe$optVariables]

## Rfe on one_hot_encoded features

rfe_one_hot <- rfe(one_hot_train_features, train_labels, 
                   sizes = c(2, 5, seq(10, 20, 2), seq(20, 50, 10), 100, 150, ncol(data)),
                   rfeControl = rfe_control,
                   metric = "f2")
one_hot_train_reduced <- one_hot_train_features[, rfe_one_hot$optVariables]
one_hot_test_reduced <- one_hot_test_features[, rfe_one_hot$optVariables]

## Rfe on numeric_features

rfe_numeric <- rfe(numeric_train_features, train_labels, 
                   sizes = c(seq(10, 50, 10), 100, 150, ncol(data)),
                   rfeControl = rfe_control,
                   metric = "f2")
numeric_train_reduced <- numeric_train_features[, rfe_numeric$optVariables]
numeric_test_reduced <- numeric_test_features[, rfe_numeric$optVariables]

```

# Models

Run the wanted model using all the combinations of the hyperparameters in the respective grid, find the optimal model and evaluate it on the test set. We try both the full-featured dataset and the reduced one

```{r, train_control, echo=TRUE}

# Use k-fold cross validation to evaluate the models
# and the appropriate summary function for the metric (f2)
ctrl <- trainControl(method = "cv", number=k_fold,
                     summaryFunction = f2_summary,
                     # summaryFunction = defaultSummary,
                     # sampling = sampling_method,
                     sampling="down",
                     verboseIter=FALSE
)

```

## Decision Tree

```{r, dt, echo=TRUE}

gs <- data.frame(cp = c(0.001, 0.005, 0.01, 0.05, 0.1))
# gs <- data.frame(cp = c(0.001))

# All features
set.seed(1)
dt_results <- run_model(gs, train_features, train_labels, test_features, test_labels, method="rpart", ctrl)
dt_results$conf_matrix
dt_results$f2


# Reduced features
set.seed(1)
dt_reduced <- run_model(gs, train_reduced, train_labels, test_reduced, test_labels, method="rpart", ctrl)
dt_reduced$conf_matrix
dt_reduced$f2

# One_hot features
set.seed(1)
dt_one_hot <- run_model(gs, one_hot_train_features, train_labels, one_hot_test_features, test_labels, method="rpart", ctrl)
dt_one_hot$conf_matrix
dt_one_hot$f2

# One_hot reduced features
set.seed(1)
dt_one_hot_reduced <- run_model(gs, one_hot_train_reduced, train_labels, one_hot_test_reduced, test_labels, method="rpart", ctrl)
dt_one_hot_reduced$conf_matrix
dt_one_hot_reduced$f2

```

## Random Forest

```{r, rf, echo=TRUE}

default <- round(sqrt(ncol(train_features)))
gs <- data.frame(mtry = c(default - 10, default - 5, default, default + 5, default + 10, default + 20))

# All features
set.seed(1)
rf_results <- run_model(gs, train_features, train_labels, test_features, test_labels, method="rf", ctrl)
rf_results$conf_matrix
rf_results$f2

# Appropriate upper limit
default <- round(sqrt(ncol(train_reduced)))
gs <- data.frame(mtry = c(default, default + 2, default + 4, default + 6))

# Reduced features
set.seed(1)
rf_reduced <- run_model(gs, train_reduced, train_labels, test_reduced, test_labels, method="rf", ctrl)
rf_reduced$conf_matrix
rf_reduced$f2

# One_hot features
set.seed(1)
rf_one_hot <- run_model(gs, one_hot_train_features, train_labels, one_hot_test_features, test_labels, method="rf", ctrl)
rf_one_hot$conf_matrix
rf_one_hot$f2

# One_hot reduced features
set.seed(1)
rf_one_hot_reduced <- run_model(gs, one_hot_train_reduced, train_labels, one_hot_test_reduced, test_labels, method="rf", ctrl)
rf_one_hot_reduced$conf_matrix
rf_one_hot_reduced$f2

```

## Support vector machines

```{r, SVM, echo=TRUE}

# One-hot encoded features
# gs <- expand.grid(C = c(1),
#                   sigma = c(1 / ncol(one_hot_train_features)),
#                   Weight = seq(0.5, 2, by=0.5))
# set.seed(1)
# svm_one_hot <- run_model(gs, one_hot_train_features, train_labels, one_hot_test_features, test_labels, method="svmRadialWeights", ctrl)
# svm_one_hot$conf_matrix
# svm_one_hot$f2


# One-hot reduced features
gs <- expand.grid(C = c(1),
                  sigma = c(1 / ncol(one_hot_train_reduced)),
                  Weight = seq(0.5, 2, by=0.5))
set.seed(1)
svm_one_hot_reduced <- run_model(gs, one_hot_train_reduced, train_labels, one_hot_test_reduced, test_labels, method="svmRadialWeights", ctrl)
svm_one_hot_reduced$conf_matrix
svm_one_hot_reduced$f2


# Numeric features
gs <- expand.grid(C = c(1),
                  sigma = c(1 / ncol(numeric_train_features)),
                  Weight = seq(0.5, 2, by=0.5))
set.seed(1)
svm_numeric <- run_model(gs, numeric_train_features, train_labels, numeric_test_features, test_labels, method="svmRadialWeights", ctrl)
svm_numeric$conf_matrix
svm_numeric$f2

# Numeric reduced features
gs <- expand.grid(C = c(1),
                  sigma = c(1 / ncol(numeric_train_reduced)),
                  Weight = seq(0.5, 2, by=0.5))
set.seed(1)
svm_numeric_reduced <- run_model(gs, numeric_train_reduced, train_labels, numeric_test_reduced, test_labels, method="svmRadialWeights", ctrl)
svm_numeric_reduced$conf_matrix
svm_numeric_reduced$f2

```

## Adaboost

```{r, adaboost, echo=TRUE}

gs <- expand.grid(mfinal = c(10, 20, 50, 100, 150),
                  maxdepth = c(1, 2),
                  coeflearn = c("Breiman"))

# All features
set.seed(1)
adaboost_results <- run_model(gs, train_features, train_labels, test_features, test_labels, method="AdaBoost.M1", ctrl)
adaboost_results$conf_matrix
adaboost_results$f2

# All features reduced
set.seed(1)
adaboost_reduced <- run_model(gs, train_reduced, train_labels, test_reduced, test_labels, method="AdaBoost.M1", ctrl)
adaboost_reduced$conf_matrix
adaboost_reduced$f2

# One_hot features
set.seed(1)
adaboost_one_hot <- run_model(gs, one_hot_train_features, train_labels, one_hot_test_features, test_labels, method="AdaBoost.M1", ctrl)
adaboost_one_hot$conf_matrix
adaboost_one_hot$f2

# One_hot reduced features
set.seed(1)
adaboost_one_hot_reduced <- run_model(gs, one_hot_train_reduced, train_labels, one_hot_test_reduced, test_labels, method="AdaBoost.M1", ctrl)
adaboost_one_hot_reduced$conf_matrix
adaboost_one_hot_reduced$f2

```

## Gradient boost

```{r, gradient_boost, echo=TRUE}

gs <- expand.grid(nrounds = c(10, 20, 50, 100),
                  max_depth = 6,
                  eta = 0.3,
                  gamma = 0,
                  colsample_bytree = 1,
                  min_child_weight = 1,
                  subsample = 1
                  )

# One_hot features
set.seed(1)
xgboost_one_hot <- run_model(gs, one_hot_train_features, train_labels, one_hot_test_features, test_labels, method="xgbTree", ctrl)
xgboost_one_hot$conf_matrix
xgboost_one_hot$f2

# One_hot reduced features
set.seed(1)
xgboost_one_hot_reduced <- run_model(gs, one_hot_train_reduced, train_labels, one_hot_test_reduced, test_labels, method="xgbTree", ctrl)
xgboost_one_hot_reduced$conf_matrix
xgboost_one_hot_reduced$f2

# Numeric features
set.seed(1)
xgboost_numeric <- run_model(gs, numeric_train_features, train_labels, numeric_test_features, test_labels, method="xgbTree", ctrl)
xgboost_numeric$conf_matrix
xgboost_numeric$f2

# Numeric reduced features
set.seed(1)
xgboost_numeric_reduced <- run_model(gs, numeric_train_reduced, train_labels, numeric_test_reduced, test_labels, method="xgbTree", ctrl)
xgboost_numeric_reduced$conf_matrix
xgboost_numeric_reduced$f2

```
